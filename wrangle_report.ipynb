{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting: wragle_report\n",
    "* Create a **300-600 word written report** called \"wrangle_report.pdf\" or \"wrangle_report.html\" that briefly describes your wrangling efforts. This is to be framed as an internal document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project goals\n",
    "This project aims at wrangling 'WeRateDogs' Twitter data, in order to create trustworhy analyses and visualizations:\n",
    "\n",
    "- Identify the most common names and breeds- all time and yearly \n",
    "- keep the tweets data with unnamed dogs to evaluate their significance\n",
    "- Identify the type of relationship between favorite_counts and retweets\n",
    "- Analyse the number of tweets, retweets an favourite_count by year\n",
    "- Create a visualization for each tweet data with the image at the center and word cloud around(Not done!! yet)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "\n",
    "### 1 Gathering data\n",
    "\n",
    "I used 3 different methods to gather the required datasets:\n",
    "\n",
    "1.Downloaded the  twitterarchive containing the : id, tweet text, rating, dog name, dog stage predictons that became the df_twitter dataframe.\n",
    "\n",
    "2.Used the Request library to gather the image-predictions.tsv file which became the df_images dataframe \n",
    "\n",
    "3.Gathered data through twitter api which required: creating a twitter developer account, where I had to explain the motivation to use their api, got the authentication key and tokens, created a api object and finally queried the Twitter API retrieving JSON data for each tweetid previously identified at the df_twitter dataframe in point 1. Each JSON tweet data was saved as a newline in txt file, a process time consuming, but once done correctly there was no need to repeat it.All tweet data in text file was then easily read into a dataframe with the desired tweet data- I choose to restrict it to tweet_id, favorites_count, retweets_count and the timestamp these pieces of data were  read into the df_twitter_api dataframe.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Data Assessment\n",
    "\n",
    "At this stage I did a programmatic and visual assessment of the dataframes in order to identify problems to be cleaned posteriorly.\n",
    "\n",
    "I used the methods head(), info(), shape(), value_count(), duplicates(), isnull().As well as observing specific columns and rows of data.\n",
    "\n",
    "As criteria followed the guidelines for quality and tidiness:\n",
    "1. looked for tweets without images or inadequate image (not of a dog)\n",
    "2. only unique tweets\n",
    "3. Mistakes: formating, incorrect breed(ex: desk)\n",
    "4. Identified structural problems, several columns that actually should be just one\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Data Cleaning\n",
    "\n",
    "Here I've followed the pattern used in the classroom, I've cleaned each dataframe by turn, some issues involved then matching the data in two dataframes. Example: I want only the tweets with image associated so I drop all the others, but then some images aren't of dog and those are removed then I need to update the previous dataframe with tweets and also remove the rows.\n",
    "\n",
    "I started by cleaning the df_twitter, then df_images, then the df_twitter_api. At the end I kept 9 tweets withou data on the retweets and favourite-count, but I considered it irrelevant amount for the retweets, favourite_count analyses but maybe relevant for tweet visualization.\n",
    "\n",
    "One issue that still remains is the possibility of some broken image links, which I tried to solve but the method did not work properly. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Merging Dataframes\n",
    "\n",
    "While merging the dataframes into the df_master dataframe, the df_twitter and df_images had the same tweet_ids but df_twitter_api had 9 tweets less, because it's a irrelavant number to the counts of retweets and favourite count, I kept those tweets as they can be interesting individual observations.\n",
    "\n",
    "The df_master was saved into csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 Exploratory Analyses and Visualizations\n",
    "\n",
    "- Histogram to visually analyse the frequency of dog_stages, including the amount of representativity of not-classified dogs.(Why so many?)\n",
    "- Most frequent breeds and names overall period \n",
    "- Scatter plot to check the type of relationship between retweets and favorite_count(positive)\n",
    "- Checked the observation(row) with max() retweets, and the nearest observations. Realized some image links were not filtered, something to be improved!!\n",
    "- Also observed the 20 largest observations in retweets. \n",
    "- Then created a column year in the df_master dataframe  from timestamp column values.\n",
    "- Analysed by year: retweets, tweets, favorite_count, names and breeds\n",
    "- Visualizations: \n",
    ".__clustered bar plot__ comparing retweets and favourite_count (the tweets frequency is not in the plot but are important in this analyses)\n",
    ".__scatter plot__ with retweets and favorite_count\n",
    ".__histogram plot__ with frequency of dog_stages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Furtherwork\n",
    "\n",
    "To allow that each observation can be expressed in a wordcloud with image kind of visualization it's necessary improve this dataframeby cleaning all the broken link images(and finding out how to do it!! exist a library wordcloud but the image should be at the center as in the visualization used for movies).\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
